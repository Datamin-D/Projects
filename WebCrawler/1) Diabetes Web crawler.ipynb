{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naver Diabetes Community Webcrawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from random import randint\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from random import *\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common import exceptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import winsound as sd\n",
    "\n",
    "\n",
    "wd = webdriver.Chrome(r\"C:\\Users\\MinSeong\\Desktop\\chrome driver\\chromedriver.exe\")\n",
    "wait = WebDriverWait(wd, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching post urls from Boards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Approach board url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url을 가져와서 뷰숲 돌리는게 빠름. Complexity!!\n",
    "\n",
    "\n",
    "\n",
    "# 상담실 url\n",
    "all_couns = 'https://cafe.naver.com/dangsamo?iframe_url=/ArticleList.nhn%3Fsearch.clubid=10096425%26search.menuid=73%26search.boardtype=L%26search.totalCount=151%26search.page=' \n",
    "noob_couns = 'https://cafe.naver.com/dangsamo?iframe_url=/ArticleList.nhn%3Fsearch.clubid=10096425%26search.menuid=682%26search.boardtype=L%26search.totalCount=151%26search.page='\n",
    "pre_couns = 'https://cafe.naver.com/dangsamo?iframe_url=/ArticleList.nhn%3Fsearch.clubid=10096425%26search.menuid=688%26search.boardtype=L%26search.totalCount=151%26search.page=' #내당능장애\n",
    "\n",
    "\n",
    "#친목 url\n",
    "type_chat = 'https://cafe.naver.com/dangsamo?iframe_url=/ArticleList.nhn%3Fsearch.clubid=10096425%26search.menuid=3%26search.boardtype=L%26search.totalCount=151%26search.page='# 유형 별 당건수다\n",
    "meal = 'https://cafe.naver.com/dangsamo?iframe_url=/ArticleList.nhn%3Fsearch.clubid=10096425%26search.menuid=285%26search.boardtype=C%26search.totalCount=101%26search.page='\n",
    "exerc = 'https://cafe.naver.com/dangsamo?iframe_url=/ArticleList.nhn%3Fsearch.clubid=10096425%26search.menuid=330%26search.boardtype=C%26search.totalCount=101%26search.page='\n",
    "\n",
    "board_urls = [all_couns,  noob_couns, pre_couns, type_chat, meal, exerc]\n",
    "# 수집할 페이지 목록\n",
    "pages = [1000,130,79,999,1000,851] # 각 게시판. #분석에 방해되는 형식 쓴 예전 글은 제외함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Approach post url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5번째 게시판 진행 중\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-4c1f04fb4c03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#url접근\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;31m# 게시판 페이지 접근\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mwd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 페이지 돌아가며 접근하기.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pages' is not defined"
     ]
    }
   ],
   "source": [
    "#원래 코드. 됨.\n",
    "\n",
    "# 피클 저장할 경로\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\MinSeong\\Desktop\\수업\\비즈니스 텍마\\Term Proj')\n",
    "\n",
    "urls = []\n",
    "\n",
    "i = 5 #0~5 \n",
    "print(f\"{i}번째 게시판 진행 중\")\n",
    "time.sleep(0.2)\n",
    "\n",
    "\n",
    "for page in tqdm(range(pages[i]+1)): #url접근\n",
    "    # 게시판 페이지 접근\n",
    "    wd.get(board_urls[i] + str(page)) # 페이지 돌아가며 접근하기.\n",
    "    \n",
    "    time.sleep(random.random())\n",
    "\n",
    "    # frame전환\n",
    "    wd.switch_to.frame('cafe_main')\n",
    "    bs = BeautifulSoup(wd.page_source, \"html.parser\")\n",
    "\n",
    "    # 게시글의 url가져오기\n",
    "    # url href를 가져와서\n",
    "   \n",
    "    #cont = bs.find_all('div',{'class':'inner_list'}) #보통은 이거로\n",
    "    #cont = bs.find_all('div',{'class':'con'}) #meal은 이거로\n",
    "    cont = bs.find_all('div',{'class':'con_top'})#exec은 이거로\n",
    "    \n",
    "    \n",
    "    for c in cont:\n",
    "        url = 'https://cafe.naver.com' + str(c.select('div > a')[0].attrs['href']) # meal은 태그 잘못 됨.\n",
    "        urls.append(url)\n",
    "\n",
    "# Beep when the code is done\n",
    "fr = 130\n",
    "dr = 1001\n",
    "sd.Beep(fr,dr)\n",
    "\n",
    "#https://m.blog.naver.com/PostView.nhn?blogId=bosongmoon&logNo=221595169882&proxyReferer=https:%2F%2Fwww.google.com%2F\n",
    "#참조함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 번째 게시판\n"
     ]
    }
   ],
   "source": [
    "# 저장 신중히\n",
    "print(i,\"번째 게시판\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신중히 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls 피클 저장 신중히\n",
    "name = 'exerc'\n",
    "\n",
    "with open(name,'wb') as fw:\n",
    "    pickle.dump(urls,fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urls피클 로드\n",
    "with open('exerc','rb') as fr:\n",
    "    u = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Iframe에서 원본 url 가져오기 --> 복잡. Selenium으로 우회."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fetching data from posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty df\n",
    "\n",
    "\n",
    "col_name = ['title','content','writer','date','view','likes','no_comments','board','url']\n",
    "df = pd.DataFrame([],columns = [col_name])\n",
    "\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\MinSeong\\Desktop\\수업\\비즈니스 텍마\\Term Proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 번째 게시판 exerc 진행중 8520 개 url.\n"
     ]
    }
   ],
   "source": [
    "# 가져올 게시판:\n",
    "i = 5 #0~5 번째 게시판 #0,1,2,3,4,5번완료 \n",
    "board_names = ['all_couns',  'noob_couns', 'pre_couns', 'type_chat', 'meal', 'exerc']\n",
    "url_skip = [81,62,61,61,0,0]\n",
    "#urls피클 로드\n",
    "with open(board_names[i],'rb') as fr:\n",
    "    urls = pickle.load(fr)\n",
    "\n",
    "# 공지 데이터 제거\n",
    "\n",
    "urls = urls[url_skip[i]:] # 80까지는 상담실 유형 게시판 공지 url. 보이는거랑 다르게. 실제 데이터 보니 공지임.\n",
    "\n",
    "\n",
    "#urls = urls[24:] # 24까지는 친목 유형 게시판 공지 url\n",
    "\n",
    "print(i,\"번째 게시판\",board_names[i],'진행중',len(urls),'개 url.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_name 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8520it [3:31:20,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "#완성본. \n",
    "\n",
    "\n",
    "#iframe에 있는 진짜 url 가져오기\n",
    "\n",
    "#wd = webdriver.Chrome(r\"C:\\Users\\MinSeong\\Desktop\\chrome driver\\chromedriver.exe\")\n",
    "\n",
    "\n",
    "troublesome_url = []\n",
    "\n",
    "for index,url in tqdm(enumerate(urls)):\n",
    "    try:\n",
    "        wd.get(url)\n",
    "        wait.until(EC.presence_of_all_elements_located)\n",
    "        time.sleep(random.random()+0.8)\n",
    "        wd.switch_to.frame('cafe_main')\n",
    "        soup = BeautifulSoup(wd.page_source,'html.parser')\n",
    "\n",
    "        #print(url)\n",
    "\n",
    "        # title\n",
    "        title = np.nan\n",
    "        try:\n",
    "            title = soup.select('h3.title_text')[0].text\n",
    "        except:\n",
    "            title = np.nan\n",
    "        #print(title)\n",
    "        # content # 다시 봐야함\n",
    "        content = np.nan\n",
    "        \n",
    "        try:\n",
    "            content = soup.select('div.se-module.se-module-text')[0].text\n",
    "        except:\n",
    "            try:\n",
    "                content = soup.select('div.ContentRenderer > div > p')[0].text\n",
    "                #print('작동',content)\n",
    "                #print(url)\n",
    "                \n",
    "                    \n",
    "            except:\n",
    "                try:\n",
    "                    contents= soup.select('div.NHN_Writeform_Main > div >div > b >span')\n",
    "                    for c in contents:\n",
    "                        empty.append(c.text)\n",
    "                        content = \"\".join(empty)\n",
    "                except:\n",
    "                    try:\n",
    "                        contents= soup.select('div.NHN_Writeform_Main > div')\n",
    "                        empty = []\n",
    "                        for c in contents:\n",
    "                            empty.append(c.text)\n",
    "                            content = \"\".join(empty)\n",
    "                    except:\n",
    "                        print(\"content nan!\")\n",
    "                        print(url)\n",
    "                        content = np.nan\n",
    "\n",
    "        #print(content)\n",
    "        #writer    \n",
    "        writer = np.nan\n",
    "        try:\n",
    "            writer = soup.select('div.nick_box > a')[0].text\n",
    "        except:\n",
    "            writer = np.nan\n",
    "        #print(writer)\n",
    "        #date\n",
    "        date = np.nan\n",
    "        try:\n",
    "            date = soup.select('span.date')[0].text\n",
    "            date = date.strip()\n",
    "        except:\n",
    "            date = np.nan\n",
    "        #print(date)\n",
    "\n",
    "        #view\n",
    "        view = np.nan\n",
    "        try:\n",
    "            view_rec = soup.select('span.count')[0].text\n",
    "            #print(view_rec)\n",
    "            p_view = re.compile('조회 ([0-9]*,?[0-9]*)')\n",
    "            view = p_view.search(view_rec).group(1)\n",
    "        except:\n",
    "            view = np.nan\n",
    "        #print(view)\n",
    "\n",
    "        #likes\n",
    "        likes = np.nan\n",
    "        try:\n",
    "            likes = soup.select('em.u_cnt._count')[0].text\n",
    "\n",
    "        except:\n",
    "            likes = np.nan\n",
    "        #print(likes)\n",
    "\n",
    "        #no_comments\n",
    "        no_comments = np.nan\n",
    "        try:\n",
    "            no_comments = soup.select('strong.num')[0].text # 두개 select됨. 같은값.\n",
    "        except:\n",
    "            no_comments = np.nan\n",
    "        #print(no_comments)\n",
    "\n",
    "        #board\n",
    "        board = board_names[i]\n",
    "        #print(board)\n",
    "\n",
    "    except:\n",
    "        troublesome_url.append(url) # 삭제된 게시물\n",
    "        \n",
    "        continue\n",
    "    df.loc[index] = title,content,writer,date,view,likes,no_comments,board,url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = board_names[i]+'.csv'\n",
    "df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exerc'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8508 entries, 0 to 8519\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   (title,)        8478 non-null   object\n",
      " 1   (content,)      6638 non-null   object\n",
      " 2   (writer,)       8478 non-null   object\n",
      " 3   (date,)         8478 non-null   object\n",
      " 4   (view,)         8478 non-null   object\n",
      " 5   (likes,)        8478 non-null   object\n",
      " 6   (no_comments,)  8478 non-null   object\n",
      " 7   (board,)        8508 non-null   object\n",
      " 8   (url,)          8508 non-null   object\n",
      "dtypes: object(9)\n",
      "memory usage: 664.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5225"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['content'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) df 합쳐주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('df_1.csv') \n",
    "df2 = pd.read_csv('df_2_4100.csv') #all_couns 0\n",
    "\n",
    "noob_couns = pd.read_csv('noob_couns.csv') #1\n",
    "\n",
    "pre_couns = pd.read_csv('pre_couns.csv') #2\n",
    "\n",
    "type_chat = pd.read_csv('type_chat.csv') #3\n",
    "\n",
    "meal = pd.read_csv('meal.csv') # 4\n",
    "\n",
    "exerc = pd.read_csv('exerc.csv')#5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.concat([df1,df2,noob_couns,pre_couns,type_chat,meal,exerc],axis = 0)\n",
    "df = df.drop(['Unnamed: 0',\t'Unnamed: 0.1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('diabetes.csv') #크롤링 완료. 결과물."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.chdir(r'C:\\Users\\MinSeong\\Desktop\\수업\\비즈니스 텍마\\Term Proj')\n",
    "\n",
    "\n",
    "df = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_couns     14984\n",
       "type_chat     14966\n",
       "meal          10000\n",
       "exerc          8508\n",
       "noob_couns     1934\n",
       "pre_couns      1181\n",
       "Name: board, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['board'].value_counts() #5만 레코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
